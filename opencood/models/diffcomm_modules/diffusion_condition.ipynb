{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "        num_classes = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "        \n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # label embeddings\n",
    "        self.label_emb = nn.Embedding(num_classes, dim)\n",
    "\n",
    "        self.label_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, labels=None, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time) # (128, 112)\n",
    "\n",
    "        if labels is not None:\n",
    "            label_emb = self.label_emb(labels)\n",
    "            label_emb = self.label_mlp(label_emb)\n",
    "            # 将类别信息与时间嵌入组合\n",
    "            t = t + label_emb\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 300\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion (using the nice property)\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_image(x_start, t):\n",
    "  # add noise\n",
    "  x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "  # turn back into PIL image\n",
    "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "  return noisy_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_losses(denoise_model, x_start, t, labels,noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t, labels = labels)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"/home/zjf/DATACENTER2/data/datasets/fashion_mnist\")\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "\n",
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "   del examples[\"image\"]\n",
    "\n",
    "   return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms)\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'pixel_values'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index, labels=None):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t, labels = labels) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, labels=None):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img,torch.full((b,), i, device=device, dtype=torch.long), i, labels)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size,  labels=None, batch_size=16, channels=3,):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size),labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,),\n",
    "    num_classes = 10\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03051385097205639\n",
      "Loss: 0.039367422461509705\n",
      "Loss: 0.031656429171562195\n",
      "Loss: 0.036858491599559784\n",
      "Loss: 0.03249533101916313\n",
      "Loss: 0.035983920097351074\n",
      "Loss: 0.029727045446634293\n",
      "Loss: 0.026309985667467117\n",
      "Loss: 0.038976795971393585\n",
      "Loss: 0.034772537648677826\n",
      "Loss: 0.03209895268082619\n",
      "Loss: 0.03349039703607559\n",
      "Loss: 0.03571981191635132\n",
      "Loss: 0.03387422859668732\n",
      "Loss: 0.029534127563238144\n",
      "Loss: 0.03263898193836212\n",
      "Loss: 0.03142388537526131\n",
      "Loss: 0.0316380150616169\n",
      "Loss: 0.03331068158149719\n",
      "Loss: 0.03354843333363533\n",
      "Loss: 0.029788879677653313\n",
      "Loss: 0.032583631575107574\n",
      "Loss: 0.02856065332889557\n",
      "Loss: 0.026203474029898643\n",
      "Loss: 0.027524923905730247\n",
      "Loss: 0.031233781948685646\n",
      "Loss: 0.027731994166970253\n",
      "Loss: 0.03639327362179756\n",
      "Loss: 0.03440631926059723\n",
      "Loss: 0.029922043904662132\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].shape[0]\n",
    "      labels = batch['label'].to(device)\n",
    "      batch = batch[\"pixel_values\"].to(device)\n",
    "      \n",
    "\n",
    "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "      loss = p_losses(model, batch, t, labels, loss_type=\"huber\")\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # save generated images\n",
    "      if step != 0 and step % save_and_sample_every == 0:\n",
    "        milestone = step // save_and_sample_every\n",
    "        batches = num_to_groups(4, batch_size)\n",
    "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
    "        all_images = torch.cat(all_images_list, dim=0)\n",
    "        all_images = (all_images + 1) * 0.5\n",
    "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605832c4386446cb5502df21f797c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### labels in Fashion-Mnist\n",
    "'''\n",
    "categories = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]\n",
    "'''\n",
    "\n",
    "# sample 64 images\n",
    "batch_size = 64\n",
    "image_size = 28 \n",
    "labels = torch.zeros(batch_size, dtype=torch.long).to(device)\n",
    "labels = torch.full((batch_size,), 9, dtype=torch.long).to(device) \n",
    "samples = sample(model, image_size=image_size, labels = labels, batch_size=64, channels=channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f594d74d910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show a random one\n",
    "random_index = 20\n",
    "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgt0lEQVR4nO3de3BU5f3H8U8SkuWWbAyQmyQY8EIVSKdcIqNQLRkuTikof3jrDLQMjjQ4RWq1dFS0diatzvTnaFH/6UCdilo7XEZnyoyCCWMLtKAMpWpKmAhYkiDRZEMCSSDP7w+GrZGLPMfd/W6W92vmzGR3zzfn2WfP7icne/a7ac45JwAAEizdegAAgMsTAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATA6wH8FW9vb06cuSIsrOzlZaWZj0cAIAn55za29tVXFys9PQLH+ckXQAdOXJEJSUl1sMAAHxDhw8f1siRIy94e9IFUHZ2dvTny/0IKMj9T2RnpUSNL9nnAbAQdB9PxOvq2bF9+fX8fOIWQKtXr9YzzzyjpqYmlZeX6/nnn9eUKVO+tu7s5KSlpRFASX7/EzW+ZJ8HoD9J1PPJOfe124rLSQivv/66VqxYoVWrVun9999XeXm5Zs2apaNHj8ZjcwCAfigtHt2wKyoqNHnyZP3+97+XdObEgpKSEj3wwAP6xS9+cdHaSCSicDjMEZCS/19P/AsOsJPs/4JzzqmtrU05OTkXXC/mR0Dd3d3avXu3Kisr/7eR9HRVVlZq+/bt56zf1dWlSCTSZwEApL6YB9CxY8d0+vRpFRQU9Lm+oKBATU1N56xfXV2tcDgcXTgDDgAuD+YfRF25cqXa2tqiy+HDh62HBABIgJifBTd8+HBlZGSoubm5z/XNzc0qLCw8Z/1QKKRQKBTrYQAAklzMj4CysrI0ceJEbdmyJXpdb2+vtmzZoqlTp8Z6cwCAfiounwNasWKFFi5cqEmTJmnKlCl69tln1dHRoR/96Efx2BwAoB+KSwDdeeed+uyzz/T444+rqalJ3/72t7V58+ZzTkwAAFy+4vI5oG8i6OeAEvlZkUR+kjjV8JkeIPWZfQ4IAIBLQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwERcumFbSGTDSppjnpGKjUVT8T4ByYojIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiZTphp3ILsZBtpWKkrkLdNDHKJnvE5BqOAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgImWakSZSkIaVydzAND092N8hQeYhUc0+k73RLE1P0Z/47q+Xuj5HQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEykTDPSZG/umKjxBWmm2dvbG2hbiWrKGmQ7GRkZ3jWSFAqFvGtOnDjhXZOoeQC+Kpka7nIEBAAwQQABAEzEPICeeOIJpaWl9VnGjh0b680AAPq5uLwHdMMNN+idd97530YGpMxbTQCAGIlLMgwYMECFhYXx+NUAgBQRl/eA9u/fr+LiYo0ePVr33nuvDh06dMF1u7q6FIlE+iwAgNQX8wCqqKjQ2rVrtXnzZr344otqaGjQtGnT1N7eft71q6urFQ6Ho0tJSUmshwQASEJpLs4fLmhtbdWoUaP0u9/9TosXLz7n9q6uLnV1dUUvRyIRlZSURE9ggJ9EzhmfAzojyOeAguBzQIiFRDwHnXNyzqmtrU05OTkXXC/uZwfk5ubq2muvVX19/XlvD4VCgZ70AID+Le6fAzp+/LgOHDigoqKieG8KANCPxDyAHnroIdXW1uqTTz7R3//+d91+++3KyMjQ3XffHetNAQD6sZj/C+7TTz/V3XffrZaWFo0YMUI333yzduzYoREjRsR6UwCAfizuJyH4ikQiCofD3ichhMNh723Nnj3bu0aSOjs7vWuys7O9a4Kckn706FHvmtbWVu8aSTp58qR3zbFjx7xrBg8e7F3z4x//2LtGkubNm+ddc/PNN3vXJPMJHEhtiThRyTmn3t7erz0JgV5wAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATMT9C+kSZdSoUd41jz76aKBtHTx40LsmMzPTu2bIkCHeNUG+nfP48ePeNVKw8RUUFHjX1NTUeNd0dHR410jBHtuhQ4d61wSdc19BG0/SxBSJwBEQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBE0nbDHjBggFcn3yuvvNJ7G93d3d41knTy5Envmq6uLu+a/fv3e9dkZGQkpEYKNg+NjY3eNV988YV3TWFhoXeNJPX09HjXnDp1KtC2gP7Ctzv6pa7PERAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATSduMtLe316sZaV5envc2gjTTlKTMzMyE1AwbNsy7ZsAA/4fUZ56/rKWlJVCdr9zc3IRsR5LKy8u9ayZNmuRd895773nXBOHbRBKpL5n2CY6AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmEjaZqS+gjSRPHXqVKBt9fb2ete0t7d71wRpYBrEF198EaguPd3/75euri7vmiDNUrOzs71rJOngwYPeNe+++653TZAGq8ePH/euCfIYScEaViZTk0v0DxwBAQBMEEAAABPeAbRt2zbNnTtXxcXFSktL08aNG/vc7pzT448/rqKiIg0aNEiVlZXav39/rMYLAEgR3gHU0dGh8vJyrV69+ry3P/3003ruuef00ksvaefOnRoyZIhmzZoV+MvfAACpyfskhDlz5mjOnDnnvc05p2effVaPPvqo5s2bJ0l6+eWXVVBQoI0bN+quu+76ZqMFAKSMmL4H1NDQoKamJlVWVkavC4fDqqio0Pbt289b09XVpUgk0mcBAKS+mAZQU1OTJKmgoKDP9QUFBdHbvqq6ulrhcDi6lJSUxHJIAIAkZX4W3MqVK9XW1hZdDh8+bD0kAEACxDSACgsLJUnNzc19rm9ubo7e9lWhUEg5OTl9FgBA6otpAJWVlamwsFBbtmyJXheJRLRz505NnTo1lpsCAPRz3mfBHT9+XPX19dHLDQ0N2rNnj/Ly8lRaWqrly5fr17/+ta655hqVlZXpscceU3FxsebPnx/LcQMA+jnvANq1a5duvfXW6OUVK1ZIkhYuXKi1a9fq4YcfVkdHh+677z61trbq5ptv1ubNmzVw4MDYjRoA0O+luSTrIBiJRBQOh5WZmenVhPKFF17w3lZpaal3jSRlZGR417S0tHjXDB061Lums7PTu2bAgGA9adva2rxrgsxDkPkOh8PeNZI0YcIE75prrrnGu+aqq67yrmltbfWuCSrJXhbQzzjn5JxTW1vbRd/XNz8LDgBweSKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmAjWBjkBMjIyvLphnzhxwnsbHR0d3jWSLvjtrhdzxRVXeNc0NTV514RCIe+aoIJ0nA7S0fnUqVPeNdnZ2d41krR+/XrvmmnTpnnXHDt2zLumoKDAu+bzzz/3rpHk9dw7K0gH7UR13Q5yf4JKxU7i8dofOAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgImmbkd52223KzMy85PXz8vK8txGk2ackdXd3e9dkZGR411x11VXeNUEadzY0NHjXSMEaFGZlZXnXDBw40LsmSLNPKVhT2/r6eu+aUaNGedds3LjRu+bWW2/1rpGk06dPB6rzlZ7u/zdwIpt9pmJj0SB85+FS1+cICABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgImkbUb62WefacCA+A6vsLAwUF2QhpVBGpjm5OR41zQ2NnrX5Ofne9dIwZqR9vT0eNccP37cuyY3N9e7RpJmzpzpXTN+/HjvmiDNUgsKCrxrNmzY4F0jST/4wQ8C1flKVLPPoNsJso/TwPTScQQEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARNI2I+3u7lZvb+8lrz906FDvbWRmZnrXSArUJDVIc8x9+/Z519xwww3eNUGbJ3700UfeNYMGDfKuSVTz16B1//nPf7xrDh065F0T5LGdNGmSd40kPfPMM941TzzxhHdNR0eHd00im30G2VZ6uv/f9T6vdd9UkAarQVzK3HEEBAAwQQABAEx4B9C2bds0d+5cFRcXKy0tTRs3buxz+6JFi5SWltZnmT17dqzGCwBIEd4B1NHRofLycq1evfqC68yePVuNjY3R5dVXX/1GgwQApB7vd9PnzJmjOXPmXHSdUCgU+NtGAQCXh7i8B1RTU6P8/Hxdd911Wrp0qVpaWi64bldXlyKRSJ8FAJD6Yh5As2fP1ssvv6wtW7bot7/9rWprazVnzhydPn36vOtXV1crHA5Hl5KSklgPCQCQhGL+OaC77ror+vP48eM1YcIEjRkzRjU1NZoxY8Y5669cuVIrVqyIXo5EIoQQAFwG4n4a9ujRozV8+HDV19ef9/ZQKKScnJw+CwAg9cU9gD799FO1tLSoqKgo3psCAPQj3v+CO378eJ+jmYaGBu3Zs0d5eXnKy8vTk08+qQULFqiwsFAHDhzQww8/rKuvvlqzZs2K6cABAP2bdwDt2rVLt956a/Ty2fdvFi5cqBdffFF79+7VH//4R7W2tqq4uFgzZ87UU089pVAoFLtRAwD6vTSXyM5+lyASiSgcDisvL8+rqd/WrVu9t3X48GHvGilYE9MrrrjCu6azs9O7pq2tLSHbCaqnp8e7prW11bsmOzvbu0aShgwZ4l0T5DNv7e3t3jXHjh3zrgk6D9dff713zdVXX+1ds27dOu+aIM1f169f710jBXuNCPLYBmlgGlQiXvKdc3LOqa2t7aLv69MLDgBgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuZfyR0r4XDYq0NskG9SDdKZWZIGDRrkXROJRLxrwuGwd83IkSO9a/71r39510jSwIEDvWuCdArOz8/3rgnSsVwK9tieOHHCu2bo0KHeNadOnfKuOXLkiHeNFOy58fHHH3vXBOk+fuONN3rXzJ0717tGCvbYBulsPW3atIRsJ9n0/3sAAOiXCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmEjaZqQVFRXKysq65PUHDx7svY2uri7vGkk6efKkd02Q5pgtLS3eNUePHvWuycjI8K6RpNbWVu+a7u5u75rs7GzvmiD7gxRsfEEaVnZ0dHjXXH/99d4148aN866Rgj03/v3vf3vXBJm7//73v941f/nLX7xrJGnSpEneNaWlpd41Tz31lHfNn/70J+8aSdq/f3+gunjgCAgAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJpG1GOnLkSIVCoUteP0hDzfHjx3vXSPIa11ltbW3eNUEaagaZh8bGRu8aSQqHwwmpOXbsmHfN6dOnvWskaeDAgd41ubm53jU9PT3eNZ9//rl3zYgRI7xrpGD7UUFBgXfNsGHDvGuCNMH1aWz8ZRUVFd41Q4cO9a6ZOHGid833v/997xopWINV51ygbX0djoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYSNpmpP/85z81YMClD6+zs9N7G0GaikpSTk6Od02QJpxBGmqeOnXKuyYtLc27RpIGDRrkXTN27Fjvmkgk4l3js+982SeffOJdE2TfCzK+zMxM75ogjTulYA01S0tLvWuC7HslJSXeNePGjfOukaT29nbvmqamJu+aII9t0Nev9HT/446gzX2/DkdAAAATBBAAwIRXAFVXV2vy5MnKzs5Wfn6+5s+fr7q6uj7rnDx5UlVVVRo2bJiGDh2qBQsWqLm5OaaDBgD0f14BVFtbq6qqKu3YsUNvv/22enp6NHPmTHV0dETXefDBB/Xmm2/qjTfeUG1trY4cOaI77rgj5gMHAPRvXu+Ebt68uc/ltWvXKj8/X7t379b06dPV1tamP/zhD1q3bp2+973vSZLWrFmjb33rW9qxY4duvPHG2I0cANCvfaP3gM5+zXReXp4kaffu3erp6VFlZWV0nbFjx6q0tFTbt28/7+/o6upSJBLpswAAUl/gAOrt7dXy5ct10003RU9xbGpqUlZWlnJzc/usW1BQcMFTE6urqxUOh6NLkFMsAQD9T+AAqqqq0r59+/Taa699owGsXLlSbW1t0eXw4cPf6PcBAPqHQJ/WW7Zsmd566y1t27ZNI0eOjF5fWFio7u5utba29jkKam5uVmFh4Xl/VygUCvyBKgBA/+V1BOSc07Jly7RhwwZt3bpVZWVlfW6fOHGiMjMztWXLluh1dXV1OnTokKZOnRqbEQMAUoLXEVBVVZXWrVunTZs2KTs7O/q+Tjgc1qBBgxQOh7V48WKtWLFCeXl5ysnJ0QMPPKCpU6dyBhwAoA+vAHrxxRclSbfcckuf69esWaNFixZJkv7v//5P6enpWrBggbq6ujRr1iy98MILMRksACB1pDnnnPUgviwSiSgcDisjI8OrUeHAgQO9t5XIJpy9vb0Jqenq6kpIjZS4+xSkeeKSJUu8ayRp8eLF3jVfPevzUgSZ8yD7eFBBtjVixAjvmhMnTnjXBNmHenp6vGukYPcpSHPazz77zLvmnnvu8a6RpJ07d3rX+L5WOufknFNbW9tFmzfTCw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCJlumEH6ZAbtBt2kCkLsq1EPTRBtxN0/pJZkP1o5syZ3jXjx4/3rrnQtwpfTJBO3ZJUUFDgXVNSUuJdE6RL9ZEjR7xrIpGId40kHTx40Lvm9ddf96758MMPvWsS+bz13dbZ9emGDQBISgQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwkbTPS9PT0lGp2mcyNRVNpns9KZKPZZG5Om8jHNsi2EtlEOIhknvNk3sdpRgoASGoEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMDLAeQKwkc9PARErF+xRE0OavyTx/yTw2KTWfg8ncWDSRfaTjNT6OgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhI2makiWy05yuZx5ZIiWrUmMj55rGFhWTf73zHd6nrcwQEADBBAAEATHgFUHV1tSZPnqzs7Gzl5+dr/vz5qqur67POLbfcorS0tD7L/fffH9NBAwD6P68Aqq2tVVVVlXbs2KG3335bPT09mjlzpjo6Ovqst2TJEjU2NkaXp59+OqaDBgD0f14nIWzevLnP5bVr1yo/P1+7d+/W9OnTo9cPHjxYhYWFsRkhACAlfaP3gNra2iRJeXl5fa5/5ZVXNHz4cI0bN04rV65UZ2fnBX9HV1eXIpFInwUAkPoCn4bd29ur5cuX66abbtK4ceOi199zzz0aNWqUiouLtXfvXj3yyCOqq6vT+vXrz/t7qqur9eSTTwYdBgCgn0pzAU9AX7p0qf7617/qvffe08iRIy+43tatWzVjxgzV19drzJgx59ze1dWlrq6u6OVIJKKSkpLoCQzxFPT3J/s5+4mSip8DAnCuoJ8DamtrU05OzgXXC3QEtGzZMr311lvatm3bRcNHkioqKiTpggEUCoUUCoWCDAMA0I95BZBzTg888IA2bNigmpoalZWVfW3Nnj17JElFRUWBBggASE1eAVRVVaV169Zp06ZNys7OVlNTkyQpHA5r0KBBOnDggNatW6fbbrtNw4YN0969e/Xggw9q+vTpmjBhQlzuAACgf/J6D+hC//Nfs2aNFi1apMOHD+uHP/yh9u3bp46ODpWUlOj222/Xo48+etH/A35ZJBJROBzmPaB+gPeAgMtDvN4DCnwSQrwQQP0HAQRcHpLqJIRUwQtb4jHnQP8T5I/NS3mu04wUAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiZRpRhqkyWV6erD8paHmGYmah3g1QuxvEnmfEtXpHGek4nzTjBQAkLQIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLpesGd7R/k2/cqSJ+soL21UrHPWKpJxccoFe8TUtOlvo4nXQC1t7dHf473E663tzeuvx+xwQtv4jHniZWq893e3q5wOHzB29Nckt3z3t5eHTlyRNnZ2ed0iI1EIiopKdHhw4eVk5NjNEJ7zMMZzMMZzMMZzMMZyTAPzjm1t7eruLj4ot86kHRHQOnp6Ro5cuRF18nJybmsd7CzmIczmIczmIczmIczrOfhYkc+Z3ESAgDABAEEADDRrwIoFApp1apVCoVC1kMxxTycwTycwTycwTyc0Z/mIelOQgAAXB761REQACB1EEAAABMEEADABAEEADDRbwJo9erVuuqqqzRw4EBVVFToH//4h/WQEu6JJ55QWlpan2Xs2LHWw4q7bdu2ae7cuSouLlZaWpo2btzY53bnnB5//HEVFRVp0KBBqqys1P79+20GG0dfNw+LFi06Z/+YPXu2zWDjpLq6WpMnT1Z2drby8/M1f/581dXV9Vnn5MmTqqqq0rBhwzR06FAtWLBAzc3NRiOOj0uZh1tuueWc/eH+++83GvH59YsAev3117VixQqtWrVK77//vsrLyzVr1iwdPXrUemgJd8MNN6ixsTG6vPfee9ZDiruOjg6Vl5dr9erV57396aef1nPPPaeXXnpJO3fu1JAhQzRr1iydPHkywSONr6+bB0maPXt2n/3j1VdfTeAI46+2tlZVVVXasWOH3n77bfX09GjmzJnq6OiIrvPggw/qzTff1BtvvKHa2lodOXJEd9xxh+GoY+9S5kGSlixZ0md/ePrpp41GfAGuH5gyZYqrqqqKXj59+rQrLi521dXVhqNKvFWrVrny8nLrYZiS5DZs2BC93Nvb6woLC90zzzwTva61tdWFQiH36quvGowwMb46D845t3DhQjdv3jyT8Vg5evSok+Rqa2udc2ce+8zMTPfGG29E1/noo4+cJLd9+3arYcbdV+fBOee++93vup/+9Kd2g7oESX8E1N3drd27d6uysjJ6XXp6uiorK7V9+3bDkdnYv3+/iouLNXr0aN177706dOiQ9ZBMNTQ0qKmpqc/+EQ6HVVFRcVnuHzU1NcrPz9d1112npUuXqqWlxXpIcdXW1iZJysvLkyTt3r1bPT09ffaHsWPHqrS0NKX3h6/Ow1mvvPKKhg8frnHjxmnlypXq7Oy0GN4FJV0z0q86duyYTp8+rYKCgj7XFxQU6OOPPzYalY2KigqtXbtW1113nRobG/Xkk09q2rRp2rdvn7Kzs62HZ6KpqUmSzrt/nL3tcjF79mzdcccdKisr04EDB/TLX/5Sc+bM0fbt25WRkWE9vJjr7e3V8uXLddNNN2ncuHGSzuwPWVlZys3N7bNuKu8P55sHSbrnnns0atQoFRcXa+/evXrkkUdUV1en9evXG462r6QPIPzPnDlzoj9PmDBBFRUVGjVqlP785z9r8eLFhiNDMrjrrruiP48fP14TJkzQmDFjVFNToxkzZhiOLD6qqqq0b9++y+J90Iu50Dzcd9990Z/Hjx+voqIizZgxQwcOHNCYMWMSPczzSvp/wQ0fPlwZGRnnnMXS3NyswsJCo1Elh9zcXF177bWqr6+3HoqZs/sA+8e5Ro8ereHDh6fk/rFs2TK99dZbevfdd/t8fUthYaG6u7vV2traZ/1U3R8uNA/nU1FRIUlJtT8kfQBlZWVp4sSJ2rJlS/S63t5ebdmyRVOnTjUcmb3jx4/rwIEDKioqsh6KmbKyMhUWFvbZPyKRiHbu3HnZ7x+ffvqpWlpaUmr/cM5p2bJl2rBhg7Zu3aqysrI+t0+cOFGZmZl99oe6ujodOnQopfaHr5uH89mzZ48kJdf+YH0WxKV47bXXXCgUcmvXrnUffvihu++++1xubq5ramqyHlpC/exnP3M1NTWuoaHB/e1vf3OVlZVu+PDh7ujRo9ZDi6v29nb3wQcfuA8++MBJcr/73e/cBx984A4ePOicc+43v/mNy83NdZs2bXJ79+518+bNc2VlZe7EiRPGI4+ti81De3u7e+ihh9z27dtdQ0ODe+edd9x3vvMdd80117iTJ09aDz1mli5d6sLhsKupqXGNjY3RpbOzM7rO/fff70pLS93WrVvdrl273NSpU93UqVMNRx17XzcP9fX17le/+pXbtWuXa2hocJs2bXKjR49206dPNx55X/0igJxz7vnnn3elpaUuKyvLTZkyxe3YscN6SAl35513uqKiIpeVleWuvPJKd+edd7r6+nrrYcXdu+++6ySdsyxcuNA5d+ZU7Mcee8wVFBS4UCjkZsyY4erq6mwHHQcXm4fOzk43c+ZMN2LECJeZmelGjRrllixZknJ/pJ3v/ktya9asia5z4sQJ95Of/MRdccUVbvDgwe722293jY2NdoOOg6+bh0OHDrnp06e7vLw8FwqF3NVXX+1+/vOfu7a2NtuBfwVfxwAAMJH07wEBAFITAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE/8P8Bt3N0lhXCAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "random_index = 53\n",
    "\n",
    "fig = plt.figure()\n",
    "ims = []\n",
    "for i in range(timesteps):\n",
    "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "animate.save('diffusion.gif')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
